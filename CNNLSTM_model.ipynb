{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrmad/anaconda3/envs/dl/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing required packages\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import callbacks\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = []\n",
    "labels = []\n",
    "test_phrase = []\n",
    "\n",
    "# load training data\n",
    "with open(\"train.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        phrase.append(row[2])\n",
    "        labels.append(row[3])\n",
    "\n",
    "# load testing data\n",
    "with open(\"test.tsv\") as testing:\n",
    "    test = csv.reader(testing, delimiter=\"\\t\", quotechar='\"')\n",
    "    for s in test:\n",
    "        test_phrase.append(s[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the sentences of the reviews.\n",
    "def clean_phrase(phrase):\n",
    "    #Remove punctuation (with a regular expression) and convert to lower case\n",
    "    words = (re.sub(\"[^a-zA-Z]\", \" \", phrase)).lower()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .',\n",
       " 'A series of escapades demonstrating the adage that what is good for the goose',\n",
       " 'A series',\n",
       " 'A',\n",
       " 'series',\n",
       " 'of escapades demonstrating the adage that what is good for the goose',\n",
       " 'of',\n",
       " 'escapades demonstrating the adage that what is good for the goose',\n",
       " 'escapades',\n",
       " 'demonstrating the adage that what is good for the goose']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the first row of the train dataset which is currently the header\n",
    "del phrase[0]\n",
    "# display first 10 rows\n",
    "phrase[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An intermittently pleasing but mostly routine effort .',\n",
       " 'An intermittently pleasing but mostly routine effort',\n",
       " 'An',\n",
       " 'intermittently pleasing but mostly routine effort',\n",
       " 'intermittently pleasing but mostly routine',\n",
       " 'intermittently pleasing but',\n",
       " 'intermittently pleasing',\n",
       " 'intermittently',\n",
       " 'pleasing',\n",
       " 'but']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the first row of the testing dataset which is currently the header\n",
    "del test_phrase[0]\n",
    "# display first 10 rows\n",
    "test_phrase[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preprocessing function on train dataset\n",
    "clean_phrases = []\n",
    "\n",
    "for x in phrase:\n",
    "    new = clean_phrase(x)\n",
    "    clean_phrases.append(new)\n",
    "    \n",
    "# run preprocessing function  on test dataset\n",
    "test_clean_phrases = []\n",
    "\n",
    "for xw in test_phrase:\n",
    "    new_test = clean_phrase(xw)\n",
    "    test_clean_phrases.append(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a series of escapades demonstrating the adage that what is good for the goose is also good for the gander   some of which occasionally amuses but none of which amounts to much of a story  ',\n",
       " 'a series of escapades demonstrating the adage that what is good for the goose',\n",
       " 'a series',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of escapades demonstrating the adage that what is good for the goose',\n",
       " 'of',\n",
       " 'escapades demonstrating the adage that what is good for the goose',\n",
       " 'escapades',\n",
       " 'demonstrating the adage that what is good for the goose']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 10 rows\n",
    "clean_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an intermittently pleasing but mostly routine effort  ',\n",
       " 'an intermittently pleasing but mostly routine effort',\n",
       " 'an',\n",
       " 'intermittently pleasing but mostly routine effort',\n",
       " 'intermittently pleasing but mostly routine',\n",
       " 'intermittently pleasing but',\n",
       " 'intermittently pleasing',\n",
       " 'intermittently',\n",
       " 'pleasing',\n",
       " 'but']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 10 rows\n",
    "test_clean_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the rows as a string with '/n' as delimiter\n",
    "all_text=' /n '.join(clean_phrases)\n",
    "\n",
    "test_all_text=' /n '.join(test_clean_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a series of escapades demonstrating the adage that what is good for the goose is also good for the g'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 100 characters\n",
    "all_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an intermittently pleasing but mostly routine effort   /n an intermittently pleasing but mostly rout'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 100 characters\n",
    "test_all_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each reviews of the training dataset and join them as a string\n",
    "reviews = all_text.split(' /n ')\n",
    "all_text = ' '.join(reviews)\n",
    "\n",
    "# split each word of the training dataset in the string to a list\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each reviews of the training dataset and join them as a string\n",
    "test_reviews = test_all_text.split(' /n ')\n",
    "test_all_text = ' '.join(test_reviews)\n",
    "\n",
    "# split each word of the training dataset in the string to a list\n",
    "test_words = test_all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'escapades',\n",
       " 'demonstrating',\n",
       " 'the',\n",
       " 'adage',\n",
       " 'that',\n",
       " 'what',\n",
       " 'is']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'intermittently',\n",
       " 'pleasing',\n",
       " 'but',\n",
       " 'mostly',\n",
       " 'routine',\n",
       " 'effort',\n",
       " 'an',\n",
       " 'intermittently',\n",
       " 'pleasing']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 156060\n",
      "Test reviews: 66292\n"
     ]
    }
   ],
   "source": [
    "# print no of rows for train and test \n",
    "print(\"Train reviews: {}\".format(len(reviews)))\n",
    "print(\"Test reviews: {}\".format(len(test_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a series of escapades demonstrating the adage that what is good for the goose is also good for the gander   some of which occasionally amuses but none of which amounts to much of a story  ',\n",
       " 'a series of escapades demonstrating the adage that what is good for the goose',\n",
       " 'a series',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of escapades demonstrating the adage that what is good for the goose',\n",
       " 'of',\n",
       " 'escapades demonstrating the adage that what is good for the goose',\n",
       " 'escapades',\n",
       " 'demonstrating the adage that what is good for the goose']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an intermittently pleasing but mostly routine effort  ',\n",
       " 'an intermittently pleasing but mostly routine effort',\n",
       " 'an',\n",
       " 'intermittently pleasing but mostly routine effort',\n",
       " 'intermittently pleasing but mostly routine',\n",
       " 'intermittently pleasing but',\n",
       " 'intermittently pleasing',\n",
       " 'intermittently',\n",
       " 'pleasing',\n",
       " 'but']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reviews[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '2', '2', '2', '2', '2', '2', '2', '2']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the first row of the labels which is currently the header\n",
    "del labels[0]\n",
    "\n",
    "# display first 10 rows\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing on the label list\n",
    "labels_cleaned = '\\n'.join(labels)\n",
    "labels_cleaned_last = labels_cleaned.split('\\n')\n",
    "\n",
    "\n",
    "len(labels_cleaned_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, ..., 3, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list to an array\n",
    "labels_sentiment = [int(i) for i in labels_cleaned_last]\n",
    "labels = np.array(labels_sentiment)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of unique labels in the labels array\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1072621"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423806"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the list that contains the individual words in the datasets\n",
    "full_words = words + test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1496427"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionaries that map the words in the vocabulary to integers. \n",
    "#Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(full_words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "#Build a dictionary that maps words to integers\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the words with integers. \n",
    "\n",
    "reviews_ints = []\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split( )])\n",
    "    \n",
    "test_reviews_ints = []\n",
    "for eachs in test_reviews:\n",
    "    test_reviews_ints.append([vocab_to_int[word] for word in eachs.split( )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17582"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check no of unique words in the corpus\n",
    "# this will be the features to be extracted\n",
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_reviews_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  315,\n",
       "  3,\n",
       "  16674,\n",
       "  7795,\n",
       "  1,\n",
       "  8458,\n",
       "  9,\n",
       "  53,\n",
       "  8,\n",
       "  47,\n",
       "  13,\n",
       "  1,\n",
       "  3992,\n",
       "  8,\n",
       "  186,\n",
       "  47,\n",
       "  13,\n",
       "  1,\n",
       "  13174,\n",
       "  61,\n",
       "  3,\n",
       "  88,\n",
       "  592,\n",
       "  12055,\n",
       "  19,\n",
       "  617,\n",
       "  3,\n",
       "  88,\n",
       "  2789,\n",
       "  5,\n",
       "  52,\n",
       "  3,\n",
       "  2,\n",
       "  42],\n",
       " [2, 315, 3, 16674, 7795, 1, 8458, 9, 53, 8, 47, 13, 1, 3992],\n",
       " [2, 315],\n",
       " [2],\n",
       " [315],\n",
       " [3, 16674, 7795, 1, 8458, 9, 53, 8, 47, 13, 1, 3992],\n",
       " [3],\n",
       " [16674, 7795, 1, 8458, 9, 53, 8, 47, 13, 1, 3992],\n",
       " [16674],\n",
       " [7795, 1, 8458, 9, 53, 8, 47, 13, 1, 3992]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ints[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 159\n",
      "Maximum review length: 48\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155901"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check total no of rows not having zero length reviews\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "len(non_zero_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zero length reviews\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "labels = np.array([labels[ii] for ii in non_zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 48\n"
     ]
    }
   ],
   "source": [
    "#check again\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155901"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As maximum review length too many steps for RNN. Let's truncate to 12 steps. \n",
    "#For reviews shorter than 12 steps, we'll pad with 0s. For reviews longer than 12 steps,\n",
    "# we will truncate them to the first 12 characters.\n",
    "\n",
    "max_review_length = 12\n",
    "X_train = sequence.pad_sequences(reviews_ints, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(test_reviews_ints, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155901, 12)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66292, 12)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  592, 12055,    19,   617,     3,    88,  2789,     5,    52,\n",
       "            3,     2,    42],\n",
       "       [    3, 16674,  7795,     1,  8458,     9,    53,     8,    47,\n",
       "           13,     1,  3992],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     2,   315],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     2],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,   315]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,   16, 2821, 1787,   19,  528, 1018,\n",
       "         396],\n",
       "       [   0,    0,    0,    0,    0,   16, 2821, 1787,   19,  528, 1018,\n",
       "         396],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          16],\n",
       "       [   0,    0,    0,    0,    0,    0, 2821, 1787,   19,  528, 1018,\n",
       "         396],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 2821, 1787,   19,  528,\n",
       "        1018]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17583\n"
     ]
    }
   ],
   "source": [
    "# check no of unique words in the corpus\n",
    "# Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "# this value will be passed to the embedding layer\n",
    "top_words = len(vocab_to_int) + 1\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding the labels\n",
    "y_train = np_utils.to_categorical(labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155901, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Callbacks\n",
    "# ModelCheckpoints is used to save the model after every epoch\n",
    "# EarlyStopping is used to stop training when the validation loss has not improved after 2 epochs\n",
    "# Tensorboard is used tovisualize dynamic graphs of the training and test metrics\n",
    "cbks = [callbacks.ModelCheckpoint(filepath='./checkpoint_model.h5', monitor='val_loss', save_best_only=True),\n",
    "            callbacks.EarlyStopping(monitor='val_loss', patience=2),callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrmad/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 12, 32)            623328    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 12, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 6, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 680,137\n",
      "Trainable params: 680,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 124720 samples, validate on 31181 samples\n",
      "Epoch 1/5\n",
      "124720/124720 [==============================] - 42s 337us/step - loss: 0.9658 - acc: 0.6119 - val_loss: 1.0204 - val_acc: 0.5865\n",
      "Epoch 2/5\n",
      "124720/124720 [==============================] - 41s 326us/step - loss: 0.7746 - acc: 0.6825 - val_loss: 1.0192 - val_acc: 0.5940\n",
      "Epoch 3/5\n",
      "124720/124720 [==============================] - 41s 329us/step - loss: 0.7046 - acc: 0.7074 - val_loss: 1.0383 - val_acc: 0.5922\n",
      "Epoch 4/5\n",
      "124720/124720 [==============================] - 41s 329us/step - loss: 0.6486 - acc: 0.7285 - val_loss: 1.0873 - val_acc: 0.5806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9524c5d2b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Model Architecture\n",
    "\n",
    "# embedding layer size\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(19479, embedding_vecor_length, input_length=max_review_length, dropout=0.2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# 1 layer of 100 units in the hidden layers of the LSTM cells\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train,validation_split=0.20, epochs=5,verbose=1, batch_size=32,callbacks=cbks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualize the training graphs\n",
    "#run \"tensorboard --logdir='./logs' \"  from the command terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "# returns a compiled model\n",
    "model = load_model('checkpoint_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model architecture\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "test_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edits the test file to input the prediction labels\n",
    "test_df = pd.read_csv('test.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Sentiment'] = test_pred.reshape(-1,1) \n",
    "header = ['PhraseId', 'Sentiment']\n",
    "test_df.to_csv('./final_predicted_model.csv', columns=header, index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
